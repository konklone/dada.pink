\documentclass{acm_proc_article-sp}
\begin{document}
\title{Automating the assessment of spreadsheet usability, data quality, and relevance}
\numberofauthors{1}
\author{ \alignauthor Thomas Levine\\ \email{\_@thomaslevine.com} }
\date{6 March 2014}
\maketitle
\begin{abstract}
When you have lots of spreadsheets, it gets hard to look through all of them.
In my research, I have been exploring methods for understanding the contents
of thousands of spreadsheets at once. I will discuss strategies for automatically
assessing the usability of spreadsheets, the quality of data
in spreadsheets, and the relevance of specific spreadsheets to particular
analysis questions; I will explain both how these methods work and how they
can help you you manage and analyze your spreadsheets.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]
\terms{Research paper}
%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}
Through recent open data initiatives, governments and large organizations have
begun releasing much of their internal data as public files on the internet.
From just a few different websites, we can easily download 100,000 different
spredsheets.\cite{ny-talk}
My research began with this question: What becomes possible when we use
quantitative methods to look at 100,000 different datasets at once?

The research has since focused more specifically on trying to understand what
is going on in data-sharing ecosystems.
I've been collecting data about publically shared open data
and looking for patterns in publishing and usage across the datasets.
Here are three specific issues that I look at.

1. How can I find datasets that I care to see?
2. What can I do about incomplete metadata?
3. Can we quantify how good a particular dataset is?

I'll briefly discuss some related work by other people and explain how I
acquire lots of spreadsheets; then I'll review some of my findings in the
above three areas.

\section{Related work}
Many other people have used relatively qualitative means to make sense of
the release of diverse open data spreadsheets.
For example, Open Knowledge Foundation volunteers manually looked through
many websites to asseble a census\cite{open-data-census}
of the availability of key datasets released by different countries.

McKinsey\cite{mckinsey} and the Governance Lab\cite{govlab}\cite{joel}
have looked at how specific businesses use specific publically available spreadsheets.

The general approach in these various studies is to look in depth at how a
few datasets are used, or how data-related projects are run. My research,
on the other hand, tries to get a broad picture across many different spreadsheets.

\section{Acquiring lots of spreadsheets}
Data catalogs make it kind of easy to get a bunch of spreadsheets all together.
The basic approach is this.

\begin{enumerate}
\item Get all of the dataset identifiers.
\item Download the metadata document about each dataset.
\item Download data files about each dataset.
\end{enumerate}

I've implemented this for the following data catalog softwares.

\begin{itemize}
\item Socrata
\item CKAN
\item Junar (partially)
\item OpenDataSoft
\end{itemize}

This allows me to get all of the data from most of the open data catalogs I know about.

Most of these spreadsheets are represented as tables,
where rows correspond to records and columns correspond to variables.\cite{table}

After I've downloaded spreadsheets and their metadata,
I assemble them into a spreadsheet about spreadsheets.\cite{data-driven}
In this super-spreadsheet, each record corresponds to a full
sub-spreadsheet; you could say that I am collecting features or statistics
about each spreadsheet.

\section{Findings}
Here are some of the things I've found by looking at lots of spreadsheets at
once. I think of them as ways of automating some of the early steps in data
analysis, but I'm packaging them into the three categories I mentioned above
(looking for datasets, dealing with metadata problems, and quantifying quality).

\subsection{New ways of looking for datasets}
We search for prose by typing prose into a search bar; why don't
we search for spreadsheets by typing spreadsheets into a search bar?
Spreadsheets are much more structured than arbitrary prose, and we
can use this structure to enable new search paradigms. We could search
for things like the following.

\begin{itemize}
\item Spreadsheets that were produced by the same program as another spreadsheet
\item Spreadsheets that I can join to a particular spreadsheet
\item Spreadsheets with a particular statistical unit
\item Spreadsheets in long format (rather than wide format)
\end{itemize}

One example is the detection of spreadsheets that can be stacked on top of
each other (unioned).
My work on this started with AppGen,\cite{appgen} which was a system to
generate random apps based on randomly combined datasets. I combined spreadsheets
by matching spreadsheets with the same column headers.

Spreadsheets with the same column headers seemed to be semantically related to
each other. For example, there were 23 spreadsheets with these same columns.
\begin{itemize}
\item type\_of\_abuse\_of\_authority\_allegation
\item substantiated\_number
\item sunstantiated\_rate
\item exonerated\_number
\item exonerated\_rate
\item unsubstantiated\_number
\item unsubstantiated\_rate
\item unfounded\_number
\item unfounded\_rate
\item officer\_unidentified\_number
\item officer\_unidentified\_rate
\item miscellaneous
\item miscellaneous\_rate
\end{itemize}

All of these spreadsheets were uploaded by the same person, and they all had
titles of the form ``\$\{crime\} Allegations \$\{year\}'', such as
``Disposition Of Force Allegations 2006''.



\subsection{Dealing with incomplete metadata}
People complain about how data are bad and metadata are bad. Rather than
fixing it on a case-by-case basis, I think we should just come up with ways
of dealing with it. As an example, let's talk about unique identifiers.

I don't know of anyone who specifies within a spreadsheet file which columns
should be unique. There are methods with external metadata files, such as data
packages\cite{datapackages}, but hardly anyone uses those either. Rather than
trying to get people to write down which columns are unique, we can just figure
it out for ourselves.

special\_snowflake\cite{special_snowflake} is a package that does just that.
It looks at all combinations of columns within a particular spreadsheet and
determines which combinations function as primary keys.

\subsection{Quantifying data quality}
A bunch of people have come up with guidelines for publishing data.

* Open Knowledge Foundation [Open Data Census](http://census.okfn.org/)
* Tim Berners-Lee [Five Stars](http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/) of open linked data.
    <!-- http://opendata.stackexchange.com/a/529 -->
* Open Government Working Group [8 Principles of Open Government Data](http://www.opengovdata.org/home/8principles)
* Sunlight Foundation [Open Data Policy Guidelines](http://sunlightfoundation.com/opendataguidelines/)
* Open Data Institute [Certificates](https://certificates.theodi.org/)

These guidelines are written in prose and in a somewhat qualitative manner.
I've been exploring ways of assigning numbers to represent data quality.
Using only the simple metadata files from open data catalogs, I've come up
with automated approaches for describing the updating,\cite{updatdness}
licensing,\cite{licensing} size,\cite{summary}, file format\cite{file-formats}
and availability\cite{dead}\cite{zombie} of groups of datasets.
Quantifying these guidelines about data quality, we can more quickly and
precisely assess data quality.

\section{Applications}
A couple of people can share a few spreadsheets without any special means,
but it gets hard when there are more than a couple people sharing more than
a few spreadsheets. In my research, I'm coming up with approaches for assisting
this sharing.

Software for the publishing and analysis of data can integrate new search
paradigms to assist people in finding relevant datasets or enriching existing
datasets.

The ubiquitous and persistent complaint of bad metadata can be tempered through
the use of tools that infer metadata or provide alternative search strategies;
by becoming more robust to bad metadata, we make available a broader range of
datasets.

Quantification of the quality of data can be helpful to those who are tasked
with cataloging and maintaining a diverse array of datasets. Data quality
statistics provide a quick and timely summary of the issues with different
datasets and allow for a more targeted approach in the maintanance of a
data catalog.

\section{Future research}
causal impacts outcomes


\subsection{Figures}
Like tables, figures cannot be split across pages; the
best placement for them
is typically the top or the bottom of the page nearest
their initial cite.  To ensure this proper ``floating'' placement
of figures, use the environment
\textbf{figure} to enclose the figure and its caption.

This sample document contains examples of \textbf{.eps}
and \textbf{.ps} files to be displayable with \LaTeX.  More
details on each of these is found in the \textit{Author's Guide}.

\begin{figure}
\centering
\epsfig{file=fly.eps}
\caption{A sample black and white graphic (.eps format).}
\end{figure}

\begin{figure}
\centering
\epsfig{file=fly.eps, height=1in, width=1in}
\caption{A sample black and white graphic (.eps format)
that has been resized with the \texttt{epsfig} command.}
\end{figure}


As was the case with tables, you may want a figure
that spans two columns.  To do this, and still to
ensure proper ``floating'' placement of tables, use the environment
\textbf{figure*} to enclose the figure and its caption.

Note that either {\textbf{.ps}} or {\textbf{.eps}} formats are
used; use
the \texttt{{\char'134}epsfig} or \texttt{{\char'134}psfig}
commands as appropriate for the different file types.

\bibliographystyle{abbrv}
\bibliography{spreadsheets}
\balancecolumns
\end{document}
