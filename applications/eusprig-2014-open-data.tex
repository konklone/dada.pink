\documentclass{acm_proc_article-sp}
\begin{document}
\title{Automating the assessment of spreadsheet usability, data quality, and relevance}
\numberofauthors{1}
\author{
\alignauthor
Thomas Levine\\
       \email{\_@thomaslevine.com}
}
\date{6 March 2014}
\maketitle
\begin{abstract}
When you have lots of spreadsheets, it gets hard to look through all of them.
In my research, I have been exploring methods for understanding the contents
of thousands of spreadsheets at once. I will discuss strategies for automatically
assessing the usability of spreadsheets, the quality of data
in spreadsheets, and the relevance of specific spreadsheets to particular
analysis questions; I will explain both how these methods work and how they
can help you you manage and analyze your spreadsheets.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]
\terms{Research paper}
%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}
Through recent open data initiatives, governments and large organizations have
begun releasing much of their internal data as public files on the internet.
From just a few different websites, we can easily download 100,000 different
spredsheets.%
\foot{Thomas Levine}{100,000 open data across 100 portal}{http://thomaslevine.com/!/data-about-open-data-talk-december-2-2013/}
My research began with this question: What becomes possible when we use
quantitative methods to look at 100,000 different datasets at once?

The research has since focused more specifically on trying to understand what
is going on in data-sharing ecosystems.
I've been collecting data about publically shared open data%
\foot{Thomas Levine}{Open Data had Better be Data-Driven}{http://thomaslevine.com/!/dataset-as-datapoint}
and looking for patterns in publishing and usage across the datasets.
Here are three specific issues that I look at.

1. How can I find datasets that I care to see?
2. What can I do about incomplete metadata?
3. Can we quantify how good a particular dataset is?

I'll briefly discuss some related work by other people and explain how I
acquire lots of spreadsheets; then I'll review some of my findings in the
above three areas.

\section{Related work}
Many other people have used relatively qualitative means to make sense of
the release of diverse open data spreadsheets.
For example, the Open Knowledge Foundation produced an
\foot{Open Knowledge Foundation}{Open Data Census}{http://census.okfn.org/}
of the availability of key datasets released by different countries.

McKinsey\foot
{James Manyika, Michael Chui, Diana Farrell, Steve Van Kuiken, Peter Groves, and Elizabeth Almasi Doshi, McKinsey Global Institute}
{Open data: Unlocking innovation and performance with liquid information}
{http://www.mckinsey.com/insights/business\_technology/open\_data\_unlocking\_innovation\_and\_performance\_with\_liquid\_information}
and the Governance Lab\foot
{Beth Simone Noveck}
{From Faith-Based to Evidence-Based: The Open Data 500 and Understanding How Open Data Helps the American Economy}
{http://www.forbes.com/sites/bethsimonenoveck/2014/01/08/from-faith-based-to-evidence-based-the-open-data-500-and-understanding-how-open-data-helps-the-american-economy/}%
\foot{Joel Gurin}{Open Data Now}{http://www.opendatanow.com/}
have looked at how specific businesses use specific publically available spreadsheets.

The general approach in these various studies is to look in depth at how a
few datasets are used, or how data-related projects are run. My research,
on the other hand, tries to get a broad picture across many different spreadsheets.

\section{Acquiring lots of spreadsheets}
Data catalogs make it kind of easy to get a bunch of spreadsheets all together.
The basic approach is this.

\begin{enumerate}
\item Get all of the dataset identifiers.
\item Download the metadata document about each dataset.
\item Download data files about each dataset.
\end{enumerate}

I've implemented this for the following data catalog softwares.

\begin{itemize}
\item Socrata
\item CKAN
\item Junar (kind of)
\item OpenDataSoft
\end{itemize}

This allows me to get all of the data from most of the open data catalogs I know about.

Most of these spreadsheets are represented as tables,
where rows correspond to records and columns correspond to variables.
http://www.datakind.org/blog/whats-in-a-table/

After I've downloaded spreadsheets and their metadata,
I assemble them into a spreadsheet about spreadsheets. %reference the socrata thing
In this super-spreadsheet, each record corresponds to a full
sub-spreadsheet; you could say that I am collecting features or statistics
about each spreadsheet.

\section{Findings}
Here are some of the things I've found by looking at lots of spreadsheets at
once. I think of them as ways of automating some of the early steps in data
analysis, but I'm packaging them into the three categories I mentioned above
(looking for datasets, dealing with metadata problems, and quantifying quality).

\subsection{New ways of looking for datasets}
We search for prose by typing prose into a search bar; why don't
we search for spreadsheets by typing spreadsheets into a search bar?
Spreadsheets are much more structured than arbitrary prose, and we
can use this structure to enable new search paradigms. We could search
for things like the following.

\begin{itemize}
\item Spreadsheets that were produced by the same program as another spreadsheet
\item Spreadsheets that I can join to a particular spreadsheet
\item Spreadsheets with a particular statistical unit
\item Spreadsheets in long format (rather than wide format)
\end{itemize}

\subsubsection{Candidates for unioning}
My work on this started with AppGen,
% * [http://appgen.me/audit/report](Datasets with the same schema)
which was a system to generate random apps based on randomly combined
datasets. As part of this, I looked for spreadsheets with the same
column headers. Spreadsheets with the same column headers seemed to be
semantically related to each other. For example, there were ten spreadsheets
with the headers ..., and these spreadsheets were all
standardized test scores.

\subsubsection{Determination of primary keys}
* [`special_snowflake`](https://pypi.python.org/pypi/special_snowflake)



\subsection{Dealing with incomplete metadata}
People complain about how data are bad and metadata are bad. Rather than
fixing it on a case-by-case basis, I think we should just come up with ways
of dealing with it.

[Missouri](/!/missouri-data-licensing/) provides a good illustration of this.
The titles of datasets are related to the contents of datasets.

You can see my alternative search approaches as ways of guessing metadata.

I found that datasets with the same column headers
tended to also have similar titles and descriptions.


\subsection{Quantifying data quality}
A bunch of people have come up with guidelines for publishing data.

* Open Knowledge Foundation [Open Data Census](http://census.okfn.org/)
* Tim Berners-Lee [Five Stars](http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/) of open linked data.
    <!-- http://opendata.stackexchange.com/a/529 -->
* Open Government Working Group [8 Principles of Open Government Data](http://www.opengovdata.org/home/8principles)
* Sunlight Foundation [Open Data Policy Guidelines](http://sunlightfoundation.com/opendataguidelines/)
* Open Data Institute [Certificates](https://certificates.theodi.org/)

These guidelines are written in prose and in a somewhat qualitative manner.
I've been exploring ways of assigning numbers to represent data quality.


These may it faster to measure things and stuff

\section{Discussion}

\section{Future research}



\subsection{Citations}
Citations to articles \cite{bowman:reasoning, clark:pct, braams:babel, herlihy:methodology},
conference
proceedings \cite{clark:pct} or books \cite{salas:calculus, Lamport:LaTeX} listed
in the Bibliography section of your
article will occur throughout the text of your article.
You should use BibTeX to automatically produce this bibliography;
you simply need to insert one of several citation commands with
a key of the item cited in the proper location in
the \texttt{.tex} file \cite{Lamport:LaTeX}.
The key is a short reference you invent to uniquely
identify each work; in this sample document, the key is
the first author's surname and a
word from the title.  This identifying key is included
with each item in the \texttt{.bib} file for your article.

The details of the construction of the \texttt{.bib} file
are beyond the scope of this sample document, but more
information can be found in the \textit{Author's Guide},
and exhaustive details in the \textit{\LaTeX\ User's
Guide}\cite{Lamport:LaTeX}.

This article shows only the plainest form
of the citation command, using \texttt{{\char'134}cite}.
This is what is stipulated in the SIGS style specifications.
No other citation format is endorsed.

\subsection{Tables}
Because tables cannot be split across pages, the best
placement for them is typically the top of the page
nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and
the table caption.  The contents of the table itself must go
in the \textbf{tabular} environment, to
be aligned properly in rows and columns, with the desired
horizontal and vertical rules.  Again, detailed instructions
on \textbf{tabular} material
is found in the \textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table 1 is included in the input file; compare the
placement of the table here with the table in the printed
dvi output of this document.

\begin{table}
\centering
\caption{Frequency of Special Characters}
\begin{tabular}{|c|c|l|} \hline
Non-English or Math&Frequency&Comments\\ \hline
\O & 1 in 1,000& For Swedish names\\ \hline
$\pi$ & 1 in 5& Common in math\\ \hline
\$ & 4 in 5 & Used in business\\ \hline
$\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
\hline\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of
the page's live area, use the environment
\textbf{table*} to enclose the table's contents and
the table caption.  As with a single-column table, this wide
table will ``float" to a location deemed more desirable.
Immediately following this sentence is the point at which
Table 2 is included in the input file; again, it is
instructive to compare the placement of the
table here with the table in the printed dvi
output of this document.


\begin{table*}
\centering
\caption{Some Typical Commands}
\begin{tabular}{|c|c|l|} \hline
Command&A Number&Comments\\ \hline
\texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
\texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
\texttt{{\char'134}table}& 300 & For tables\\ \hline
\texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
\end{table*}
% end the environment with {table*}, NOTE not {table}!

\subsection{Figures}
Like tables, figures cannot be split across pages; the
best placement for them
is typically the top or the bottom of the page nearest
their initial cite.  To ensure this proper ``floating'' placement
of figures, use the environment
\textbf{figure} to enclose the figure and its caption.

This sample document contains examples of \textbf{.eps}
and \textbf{.ps} files to be displayable with \LaTeX.  More
details on each of these is found in the \textit{Author's Guide}.

\begin{figure}
\centering
\epsfig{file=fly.eps}
\caption{A sample black and white graphic (.eps format).}
\end{figure}

\begin{figure}
\centering
\epsfig{file=fly.eps, height=1in, width=1in}
\caption{A sample black and white graphic (.eps format)
that has been resized with the \texttt{epsfig} command.}
\end{figure}


As was the case with tables, you may want a figure
that spans two columns.  To do this, and still to
ensure proper ``floating'' placement of tables, use the environment
\textbf{figure*} to enclose the figure and its caption.

Note that either {\textbf{.ps}} or {\textbf{.eps}} formats are
used; use
the \texttt{{\char'134}epsfig} or \texttt{{\char'134}psfig}
commands as appropriate for the different file types.

\subsection*{A {\secit Caveat} for the \TeX\ Expert}
Because you have just been given permission to
use the \texttt{{\char'134}newdef} command to create a
new form, you might think you can
use \TeX's \texttt{{\char'134}def} to create a
new command: \textit{Please refrain from doing this!}
Remember that your \LaTeX\ source code is primarily intended
to create camera-ready copy, but may be converted
to other forms -- e.g. HTML. If you inadvertently omit
some or all of the \texttt{{\char'134}def}s recompilation will
be, to say the least, problematic.

\section{Conclusions}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
% \bibliographystyle{abbrv}
% \bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
\balancecolumns
\end{document}
