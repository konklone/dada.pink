For the past few years, I've been hearing a lot about "open data".
I was apparently even an expert in open data even though I didn't
really know what it was. So I started going to various repositories
of open data, downloading all of the data, and looking at what they
contained.

(plot of dataset counts by site)

With hardly any effort, I had amassed tens of thousands of datasets,
I quickly realized that nobody had any idea what was in these datasets,
so I started making tools to understand what was in them.

Before I talk about these tools, let me explain why nobody knows what's
in these datasets. Also, keep in mind that "dataset" is usually a fancy
word for "spreadsheet".

(slide)

How do we figure out what's in a bunch of spreadsheets today?
We could manually look at every cell in every spreadsheet,
but this would take a long time.

(slide)

Many "open data" guidelines provide direction as to how a person
or organization with lots of datasets might allow other people
to use them. At a basic level, many guidelines suggest that data
be available on the internet and under a free license; at the other
end of the spectrum, guidelines suggest that data be in standard
formats accompanied with assorted metadata. These guidelines put
responsibility on the publishers of data to do certain things when
they publish data. This can be a lot of work, so it's unreasonable
for us to expect this to happen for anything other than the most
important of datasets.

(slide)

% Many groups have developed standard schemas for particular sorts
% of data. examples (cite stuff) RSS, Yelp, other CfA, geojson

This is helpful when different organizations have similar datasets.
For example, this can work if multiple bloggers are publishing blog
posts or if multiple cities are publishing restaurant inspection results.
It still puts a burden on the publishers of data.

(slide)

% Graphs and stuff
% Even if we have the most perfect graphs about each separate spreadsheet,
% we won't have time to look at them all; we need to narrow down
% the spreadsheet base before we look at them

(slide)

In practice, I find that people learn what's in a spreadsheet through word
of mouth, without looking directly at the spreadsheet. I know what's in the
various datasets published by the United States Census, but I've never used
any of them myself. I know what's in New York City's tax parcel boundaries
dataset (PLUTO) because lots of people have been talking about it,
but I haven't looked at it myself.
http://spatialityblog.com/2013/04/04/a-modest-proposal-for-nyc-tax-parcel-data/
http://www.codeforamerica.org/blog/2013/07/25/epic-win-for-nycs-open-data-community-pluto-is-free/
And I know that several cities release data about the use of bike-share programs
because people get excited every time this happens, but again, I haven't looked
at any of it myself.

(slides with pictures of people)

The people I meet who know the most about what's in these large collections
of disparate open data spreadsheets are the people in charge of putting them
on the internet in a form that people can use them. 

People like Nicole, Noel, Jack, Chris, Mark (above)
have worked with various organizations (mostly governments) to publish
their data, and they know a lot the many datasets being published by the
organizations they respectively work. If you want to know whether a city
publishes a particular dataset, these are the people you want to ask.

Unfortunately, these people don't have time to serve as everyone's personal
search engine, and there isn't an easy way for them to convey all of this
knowledge to others. Moreover, they only know about the data they've worked
on in their respective governments; they don't know much about other governments.

(slide)

Considering that we're talking about open **data**, I figured that we should
be searching through these data in a **data-driven** way. As I said earlier,
I started downloading lots of datasets and running crude statistics across them.

(slide)

(slide)

(slide)

These "datasets" are mostly in comma-separated values (CSV) format---spreadsheets
without code---where columns are variables and rows are records. It turns out that
most of them are pretty small.

(slide)

(slide)

Many organizations report the number of datasets that they publish, and this number
turns out to be nearly useless.

(slide)

I looked at how people use different features of these data publishing website
software programs. One of these programs is called "Socrata Open Data Portal",
and it has features for making graphs of data inside your browser. If you save
the graph you make, it shows up on the public website, so I can see how people
are using this feature. To make the long story short, it looks like people don't
use this feature very much.

(slide about file formats)

I started looking into the file formats that people use....

(slide about duplication)

other things....

My main approach in all of this work has been to acquire a lot of spreadsheets
and run silly things across all of them. At some point, I started focusing this
a bit. People have various ideas about what makes for a good dataset, and these
are codified in the various guidelines that I mentioned before. The guidelines
I reference are all written in English, and none are written in code. I started
coming up with software approaches for assessing compliance with these guidelines.

updatedness

dead links

I continue to be shocked that people find any of this stuff interesting.
For the various statistics above, I haven't been doing any notable inspection
of the contents of the spreadsheets. People haven't looked at these
spreadsheets in mass, so even the simplest of studies are informative.

(slide: Searching)

I gradually noticed that it's very hard to find spreadsheets that are relevant
to a particular analysis unless you already know that the spreadsheet exists.
You can search in a generic web search engine, but the spreadsheets might not
be indexed, and the search might not work that well even if the spreadsheets
are indexed.

(slide of search bar)

You can use the search bars on these various websites, but they usually do a
very naive keyword search. Also, these sites tend to be quite slow.

(slide)

Having assembled a spreadsheet about my spreadsheets, I was quite able to
search through them. Even though I only had metadata in this spreadsheet,
I could do powerful searches because I could create any metadata I wanted.
Here's an example. In my composition of music from spreadsheets and in my
teaching, I often want a dataset whose schema has particular characteristics.
So I collected data about the column types in these various tables and ran
queries like this on my spreadsheet.

(slide: A search engine for spreadsheets)

Here's another issue with using website-specific search bars.
When I'm looking for spreadsheets, the publishing organization is unlikely
to be my main concern. For example, if I'm interested in data about the
composition of different pesticides, but I don't really care whether the
data were collected by this city government or by that country government.

(middle layer slide)

In my view, we can have one set of tools/people that focus on making
data available in a crude form and another set for assembling these crude
data into something more relevent to the people who want the data

(openprism slide)

OpenPrism


Let me reiterate my thoughts on searching spreadsheets.
I see two main issues in the common means of searching through spreadsheets.
The first issue is that the search method is quite naive; these websites are
usually running keyword searches.
The second issue is that the search is localized to datasets that are published
or otherwise managed by a particular entity; it's hard to search for
spreadsheets without first identifying a specific publisher or repository.

I started to address the first issue by pulling schema-related features out
of the spreadsheets, and I started to address the second issue by letting
people search many sites at once. Taking this further, I've been thinking
about what it would mean to have a search engine for spreadsheets.

When we search for ordinary written documents, we send words into a search
engine and get pages of words back. What if we could search for spreadsheets
by sending spreadsheets into a search engine and getting spreadsheets back?
The order of the results would be determined by various specialized statistics;
just as we use PageRank to find relevant hypertext documents, we can develop
other statistics that help us find relevant spreadsheets.

So now I'm looking for ways to do interesting searches on spreadsheets.

rows and columns

special snowflake

If we pass the iris dataset to it, ::

    "Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    5.1,3.5,1.4,0.2,"setosa"
    4.9,3,1.4,0.2,"setosa"
    4.7,3.2,1.3,0.2,"setosa"
    4.6,3.1,1.5,0.2,"setosa"
    ...

we get no unique keys ::

    >>> special_snowflake.fromcsv(open('iris.csv'))                                                                  
    set()

because no combination of columns uniquely identifies the rows.
Of course, if we add an identifier column, ::

    "Id","Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    1,5.1,3.5,1.4,0.2,"setosa"
    2,4.9,3,1.4,0.2,"setosa"
    3,4.7,3.2,1.3,0.2,"setosa"
    4,4.6,3.1,1.5,0.2,"setosa"
    ...

that one gets returned. ::

    >>> special_snowflake.fromcsv(open('iris.csv'))                                                                  
    {('',)}

For a more interesting example, let's look at chickweight.

    "weight","Time","Chick","Diet"
    42,0,"1","1"
    51,2,"1","1"
    59,4,"1","1"
    64,6,"1","1"
    76,8,"1","1"
    ...

I could read the documentation on this dataset and tell you
what its statistical unit is (`?ChickWeight` in R), or I could
just let `special_snowflake` figure it out for me.

    >>> special_snowflake.fromcsv(open('chick.csv'))
    {('Time', 'Chick')}

The statistical unit is chicks in time. That is, something was
observed across multiple chick, and multiple observations were
taken from each (well, at least one) chick.

Some spreadsheets are have less obvious identifiers. In this
table of 1219 rows and 33 columns,

    >>> from urllib.request import urlopen
    >>> url = 'http://data.iledefrance.fr/explore/dataset/liste-des-points-de-contact-du-reseau-postal-dile-de-france/download/?format=csv'
    >>> fp = urlopen(url)
    >>> special_snowflake.fromcsv(fp, delimiter = ';')
    {('adresse', 'code_postal'),
     ('adresse', 'localite'),
     ('identifiant',),
     ('libelle_du_site',),
     ('wgs84',)}

we find five functional unique keys. Just by looking at the column names,
I'm gussing that the first two are combinations of parts of the postal address
and that the latter three look are formal identifiers.
And when I do things correctly and look at the
`data dictionary <http://data.iledefrance.fr/api/datasets/1.0/liste-des-points-de-contact-du-reseau-postal-dile-de-france/attachments/laposte_description_champs_pointdecontact_pdf/>`_,
I come to the same interpretation.

This tells me that this dataset is about postal service locations,
with one location per row. It also gives me some ideas as to things that can
act as unique identifiers for postal service locations.

It's kind of cool to run this on individual spreadsheets, but it's even cooler
to run this on lots of spreadsheets.

blizzard

diagrams of the network

Let's now trace what happens when we compare a new spreadsheet to our database
of spreadsheets.
