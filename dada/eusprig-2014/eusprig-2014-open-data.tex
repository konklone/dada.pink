% Adjustments
% ===========


% \subsection{Existing means of understanding a lot of spreadsheets}
% We could manually look at every cell in every spreadsheet,
% but this would take a long time. Instead, we develop tools,
% and standard practices to make this a bit easier.

% Many "open data" guidelines provide direction as to how a person
% or organization with lots of spreadsheets might allow other people
% to use them. At a basic level, many guidelines suggest that data
% be available on the internet and under a free license; at the other
% end of the spectrum, guidelines suggest that data be in standard
% formats accompanied with assorted metadata.

% Many groups have developed standard schemas for particular sorts
% of data. examples (cite stuff)

% \subsection{Things that don't exist but maybe should}
% The various tools that I describe above assist in understanding
% individual spreadsheets. What if we have a million spreadsheets?
% Even if we have the most perfect graphs about each separate spreadsheet,
% we won't have time to look at them all; we need to narrow down
% the spreadsheet base before we look at them

% It's like the concept of information retrieval and the-other-thing

% but I think we can take a qualitatively
% different approach to understanding 


% search engine

% stats for search engine (pagerank for spreadsheets)



% -- Do I need this part?
% People want to share spreadsheets with each other (citations).
% People, governments, and other organizations even want to share spreadsheets and with the
% entire world, and they often call this "open data".
% In either case, this might intuitively seem like a good idea;
% this can enable a wide range of people to perform new sorts of analysis.

% 

\documentclass{acm_proc_article-sp}
\usepackage{url}
\begin{document}
\title{How can we figure out what is inside thousands of spreadsheets?}
\numberofauthors{1}
\author{ \alignauthor Thomas Levine\\ \email{\_@thomaslevine.com} }
\date{6 March 2014}
\maketitle
\begin{abstract}
When you have lots of spreadsheets, it gets hard to look through all of them.
In my research, I have been exploring methods for understanding the contents
of thousands of spreadsheets at once. I will discuss strategies for automatically
assessing the usability of spreadsheets, the quality of data
in spreadsheets, and the relevance of specific spreadsheets to particular
analysis questions; I will explain both how these methods work and how they
can help you you manage and analyze your spreadsheets.
\end{abstract}

\keywords{data management, spreadsheets, open data}

\section{Introduction}
These days, we have more data than we know what to do with. And by "data",
we often mean unclean, poorly documented spreadsheets. How are we to have
any idea what these millions of spreadsheets contain? In my research,
I search for approaches to understanding the contents of large collections
of weakly structured spreadsheets.

\section{Typical approaches to exploring the contents of spreadsheets}
Before we discuss my experimental spreadsheet exploration methods,
let's consider some more common approaches.
How do we figure out what's in a bunch of spreadsheets today?

\subsection{Look at every spreadsheet}
One approach is to look manually at every cell in every spreadsheet.
This takes a long time, but it is feasible in some situations.

For example, the Open Knowledge Foundation \cite{open-data-census}
and Code for America \cite{open-data-census-us}
conducted open data censuses to determine which governments were
releasing what data publically on the internet. The two studies
focused on a few datasets (11 for the former, 17 for the latter)
whose public release they deemed especially important; these datasets
included budgets, laws, and procurement contracts.
In each case, volunteers searched the internet and talked to
government employees in order to determine whether each dataset was
available. When datasets were available, volunteers recorded some
metadata about the dataset and rated how well the data were published
based on a data availability rubric.

As slow as it may be, a crude approach like this one is feasible and
worthwhile when we are very focused with our goals.

\subsection{Use standard metaformats}
Many groups develop domain-specific metaformats for expressing a very specific
sort of data. For example, JSON API is a metaformat for expressing the
response of a database query on the web \cite{jsonapi}, Data Packages is a
metaformat for expressing metadata about a dataset \cite{datapackages},
and KML is a metaformat for expressing annotations of geographic maps \citep{kml}.
% http://portal.opengeospatial.org/files/?artifact_id=27810
% http://dataprotocols.org/data-packages/
% http://jsonapi.org/
% http://www.opengeospatial.org/standards/is

Agreement on format and metaformat makes it faster and easier to inspect
individual files. On the other hand, it is less helpful at alleviating the
need to acquire lots of different files and to at least glance at them.

The same sort of thing happens when data publishers provide graphs of
each individual dataset. When we provide some graphs of a dataset
rather than simply the standard data file, we try to make it easier for
people to understand that particular dataset. We are less concerned with
alleviating the need to look through many different datasets.

\subsection{Provide good metadata}
Data may be easier to find if we catalog our data well and adhere to
certain data quality standards. With this reasoning,
many "open data" guidelines provide direction as to how a person
or organization with lots of datasets might allow other people
to use them.
\cite{open-data-census,fivestars,sunlight,sebastopol,odi}

At a basic level, these guidelines suggest that data should
be available on the internet and under a free license; at the other
end of the spectrum, guidelines suggest that data be in standard
formats accompanied with particular metadata.

Datasets can be a joy to work with when these data quality guidelines
are followed, but this requires much upfront work by the publishers
of the data.





quantitative methods to look at 100,000 different datasets at once?

The research has since focused more specifically on trying to understand what
is going on in data-sharing ecosystems.
I've been collecting data about publicly shared open data
and looking for patterns in publishing and usage across the datasets.
Here are three specific issues that I look at.

\begin{enumerate}
\item How can I find datasets that I care to see?
\item What can I do about incomplete metadata?
\item Can we quantify how good a particular dataset is?
\end{enumerate}

I'll briefly discuss some related work by other people and explain how I
acquire lots of spreadsheets; then I'll review some of my findings in the
above three areas.

\section{Related work}
Many other people have used relatively qualitative means to make sense of
the release of diverse open data spreadsheets.
For example, Open Knowledge Foundation volunteers manually looked through
many websites to assemble a census \cite{open-data-census}
of the availability of key datasets released by different countries.

McKinsey \cite{mckinsey} and the Governance Lab \cite{govlab,joel}
have looked at how specific businesses use specific publicly available spreadsheets.

The general approach in these various studies is to look in depth at how a
few datasets are used, or how data-related projects are run. My research,
on the other hand, tries to get a broad picture across many different spreadsheets.

\section{Acquiring lots of spreadsheets}
Data catalogs make it kind of easy to get a bunch of spreadsheets all together.
The basic approach is this.

\begin{enumerate}
\item Get all of the dataset identifiers.
\item Download the metadata document about each dataset.
\item Download data files about each dataset.
\end{enumerate}

I've implemented this for the following data catalog softwares.

\begin{itemize}
\item Socrata
\item CKAN
\item Junar (partially)
\item OpenDataSoft
\end{itemize}

This allows me to get all of the data from most of the open data catalogs I know about.

Most of these spreadsheets are represented as tables,
where rows correspond to records and columns correspond to variables. \cite{table}

After I've downloaded spreadsheets and their metadata,
I assemble them into a spreadsheet about spreadsheets. \cite{data-driven}
In this super-spreadsheet, each record corresponds to a full
sub-spreadsheet; you could say that I am collecting features or statistics
about each spreadsheet.

\section{Findings}
Here are some of the things I've found by looking at lots of spreadsheets at
once. I think of them as ways of automating some of the early steps in data
analysis, but I'm packaging them into the three categories I mentioned above
(looking for datasets, dealing with metadata problems, and quantifying quality).

\subsection{New ways of looking for datasets}
We search for prose by typing prose into a search bar; why don't
we search for spreadsheets by typing spreadsheets into a search bar?
Spreadsheets are much more structured than arbitrary prose, and we
can use this structure to enable new search paradigms. We could search
for things like the following.

\begin{itemize}
\item Spreadsheets that were produced by the same program as another spreadsheet
\item Spreadsheets that I can join to a particular spreadsheet
\item Spreadsheets with a particular statistical unit
\item Spreadsheets in long format (rather than wide format)
\end{itemize}

One example is the detection of spreadsheets that can be stacked on top of
each other (unioned).
My work on this started with AppGen,\cite{appgen} which was a system to
generate random apps based on randomly combined datasets. I combined spreadsheets
by matching spreadsheets with the same column headers.

Spreadsheets with the same column headers seemed to be semantically related to
each other. For example, there were 23 spreadsheets with these same columns.
\begin{itemize}
\item type\_of\_abuse\_of\_authority\_allegation
\item substantiated\_number
\item sunstantiated\_rate
\item exonerated\_number
\item exonerated\_rate
\item unsubstantiated\_number
\item unsubstantiated\_rate
\item unfounded\_number
\item unfounded\_rate
\item officer\_unidentified\_number
\item officer\_unidentified\_rate
\item miscellaneous
\item miscellaneous\_rate
\end{itemize}

All of these spreadsheets were uploaded by the same person, and they all had
titles of the form ``\$\{crime\} Allegations \$\{year\}'', such as
``Disposition Of Force Allegations 2006''.

When we organize spreadsheets by their column headers, groups of related
spreadsheets pop out at us.

\subsection{Dealing with incomplete metadata}
People complain about how data are bad and metadata are bad. Rather than
fixing it on a case-by-case basis, I think we should just come up with ways
of dealing with it. As an example, let's talk about unique identifiers.

I don't know of anyone who specifies within a spreadsheet file which columns
should be unique. There are methods with external metadata files, such as data
packages \cite{datapackages}, but hardly anyone uses those either. Rather than
trying to get people to write down which columns are unique, we can just figure
it out for ourselves.

special\_snowflake \cite{specialsnowflake} is a package that does just that.
It looks at all combinations of columns within a particular spreadsheet and
determines which combinations function as unique identifiers.

With tools like special\_snowflake, we don't have to rely as much on other
people for the creation of accurate metadata; we can lazily figure out the
metadata ourselves.

\subsection{Quantifying data quality}
A bunch of people \cite{open-data-census,fivestars,sunlight,sebastopol,odi}
have come up with relatively qualitative guidelines for publishing data.

I've been exploring ways of assigning numbers to represent data quality.
Using only the simple metadata files from open data catalogs, I've come up
with automated approaches for describing the updating, \cite{updatedness}
licensing, \cite{licensing} size \cite{summary}, file format \cite{file-formats}
and availability \cite{dead,zombie} of groups of datasets.

By quantifying these guidelines about data quality, we can more quickly and
precisely assess data quality.

\section{Applications}
A couple of people can share a few spreadsheets without any special means,
but it gets hard when there are more than a couple people sharing more than
a few spreadsheets. In my research, I'm coming up with approaches for assisting
this sharing.

Software for the publishing and analysis of data can integrate new search
paradigms to assist people in finding relevant datasets or enriching existing
datasets.

The ubiquitous and persistent complaint of bad metadata can be tempered through
the use of tools that infer metadata or provide alternative search strategies;
by becoming more robust to bad metadata, we make available a broader range of
datasets.

Quantification of the quality of data can be helpful to those who are tasked
with cataloging and maintaining a diverse array of datasets. Data quality
statistics provide a quick and timely summary of the issues with different
datasets and allow for a more targeted approach in the maintenance of a
data catalog.

\section{Future}
Many people say that releasing open data will create transparency in government,
engage citizens in their governments, and stimulate the economy. This all seems
reasonable, but I haven't seen much empirical research that suggests that the
sharing of data affects any of these outcomes.

As I come up with more ways of computationally describing datasets, I am becoming
more able to apply various quantitative analyses to the dataset of datasets;
this is starting to enable weaker versions of the aforementioned measure of
outcomes. It would be nice to know whether releasing your organization's data on
the internet will get people to use them.

\bibliographystyle{abbrv}
\bibliography{spreadsheets}
\balancecolumns
\end{document}
