\documentclass{acm_proc_article-sp}
\usepackage{url}
\begin{document}
\title{How can we figure out what is inside thousands of spreadsheets?}
\numberofauthors{1}
\author{ \alignauthor Thomas Levine\\ \email{\_@thomaslevine.com} }
\date{6 March 2014}
\maketitle
\begin{abstract}
We have enough data today that we it may not be realistic to understand
all of them. In hopes of vaguely understanding these data, I have been
developing methods for exploring the contents of large collections of
weakly structured spreadsheets. We can get some feel for the contents
of these collections by assembling metadata about many spreadsheets and
run otherwise typical analyses on the data-about-data; this gives us some
understanding patterns in data publishing and a crude understanding of
the contents. I have also developed spreadsheet-specific search tools
that try to find related spreadsheets based on similarities in implicit
schema. By running crude statistics across many disparate datasets,
we can learn a lot about unweildy collections of poorly structured data.
\end{abstract}

\keywords{data management, spreadsheets, open data, search}

\section{Introduction}
These days, we have more data than we know what to do with. And by "data",
we often mean unclean, poorly documented spreadsheets. I started wondering
what was in all of these spreadsheets. Addressing my curiosity turned out
to be quite difficult, so I've found up developing various approaches to
understanding the contents of large collections of weakly structured
spreadsheets.

My initial curiosity stemmed from the release of thousands of spreadsheets
in government open data initiatives. I wanted to know what they had released
so that I may find interesting things in it.

More practically, I often am looking for data from multiple sources that I
can connect in relation to a particular topic. For example, in a project I had
data about cash flows through the United States treasury and wanted to join
them to data about the daily interest rates for United States bonds. In situations
like this, I usually need to know the name of the dataset or to ask around until
I find the name. I wanted a faster and more systematic approach to this.

\section{Typical approaches to exploring the contents of spreadsheets}
Before we discuss my spreadsheet exploration methods, let's discuss some
more ordinary methods that I see in common use today.

\subsection{Look at every spreadsheet}
As a baseline,
one approach is to look manually at every cell in many spreadsheets.
This takes a long time, but it is feasible in some situations.

\subsection{Use standard metaformats}
Many groups develop domain-specific metaformats for expressing a very specific
sort of data. For example, JSON API is a metaformat for expressing the
response of a database query on the web \cite{jsonapi}, Data Packages is a
metaformat for expressing metadata about a dataset \cite{datapackages},
and KML is a metaformat for expressing annotations of geographic maps.\citep{kml}

Agreement on format and metaformat makes it faster and easier to inspect
individual files. On the other hand, it does not alleviate the
need to acquire lots of different files and to at least glance at them.
We spend less time manually inspecting each dataset, but we must still
manually inspect lots of dataset.

The same sort of thing happens when data publishers provide graphs of
each individual dataset. When we provide some graphs of a dataset
rather than simply the standard data file, we are trying to make it easier for
people to understand that particular dataset, rather than trying to focus
them on a particular subset of datasets.

\subsection{Provide good metadata} \label{guidelines}
Data may be easier to find if we catalog our data well and adhere to
certain data quality standards. With this reasoning,
many "open data" guidelines provide direction as to how a person
or organization with lots of datasets might allow other people
to use them.
\cite{open-data-census,fivestars,sunlight,sebastopol,odi}

At a basic level, these guidelines suggest that data should
be available on the internet and under a free license; at the other
end of the spectrum, guidelines suggest that data be in standard
formats accompanied with particular metadata.

Datasets can be a joy to work with when these data quality guidelines
are followed, but this requires much upfront work by the publishers
of the data.

\subsection{Asking people}
In practice, I find that people learn what's in a spreadsheet through word
of mouth, even if the data are already published on the internet in standard
formats with good metadata.

Amanda Hickman teaches journalism and keeps a list of data sources for her
students. \cite{amanda}
https://github.com/amandabee/cunyjdata/wiki/Where-to-Find-Data

There entire conferences about the contents of newly released datasets,
such as the annual meeting of the Association of Public Data Users.\cite{apdu}

The Open Knowledge Foundation \cite{open-data-census}
and Code for America \cite{open-data-census-us}
even conducted data censuses to determine which governments were
releasing what data publically on the internet.
In each case, volunteers searched the internet and talked to
government employees in order to determine whether each dataset was
available and to collect certain information about each dataset.

\section{How I acquire lots of spreadsheets} \label{acquire}
In order to explore methods for examining thousands of spreadsheets,
I needed to find spreadsheets that I could explore.

Many governments and other large organizations publish spreadsheets on
data catalog websites.
Data catalogs make it kind of easy to get a bunch of spreadsheets all together.
The basic approach is this.

\begin{enumerate}
\item Download a list of all of the dataset identifiers that are present in the data catalog.
\item Download the metadata document about each dataset.
\item Download data files about each dataset.
\end{enumerate}

I've implemented this for the following data catalog softwares.

\begin{itemize}
\item Socrata Open Data Portal
\item Common Knowledge Archive Network (CKAN)
\item OpenDataSoft
\end{itemize}

This allows me to get all of the data from most of the open data catalogs I know about.

After I've downloaded spreadsheets and their metadata,
I often assemble them into a spreadsheet about spreadsheets. \cite{data-driven}
In this super-spreadsheet, each record corresponds to a full
sub-spreadsheet; you could say that I am collecting features or statistics
about each spreadsheet.

\section{Crude statistics about spreadsheets}
My first approach was involved running rather crude analyses on this
interesting dataset-about-datasets that I had assembled.

\subsection{How many datasets}
I started out by simply counting how many datasets each catalog website had.

{../socrata-summary/figure/big_portals_datasets.png}

The smaller sites had just a few spreadsheets, and the larger sites had thousands.

\subsection{Meaninglessness of the count of datasets}
Many organizations report this count of datasets that they publish, and this number
turns out to be nearly useless. As illustration of this, let's consider a specific
group of spreadsheets. Here are the titles of a few spreadsheets in New York City's
open data catalog.

\begin{itemize}
\item Math Test Results 2006-2012 - Citywide - Gender
\item Math Test Results 2006-2012 - Citywide - Ethnicity
\item English Language Arts (ELA) Test Results 2006-2012 - Citywide - SWD
\item English Language Arts (ELA) Test Results 2006-2012 - Citywide - ELL
\item Math Test Results 2006-2012 - Citywide - SWD
\item English Language Arts (ELA) Test Results 2006-2012 - Citywide - All Students
\item Math Test Results 2006-2012 - Citywide - ELL
\item English Language Arts (ELA) Test Results 2006-2012 - Citywide - Gender
\item Math Test Results 2006-2012 - Citywide - All Students
\item English Language Arts (ELA) Test Results 2006-2012 - Citywide - Ethnicity
\end{itemize}

These spreadsheets all had the same column names; they were

\begin{itemize}
\item grade
\item year
\item demographic
\item number_tested
\item mean_scale_score
\item num_level_1
\item pct_level_1
\item num_level_2
\item pct_level_2
\item num_level_3
\item pct_level_3
\item num_level_4
\item pct_level_4
\item num_level_3_and_4
\item pct_level_3_and_4
\end{itemize}

These "datasets" can all be thought of as subsets of the same single dataset.
If I just take different subsets of a single spreadsheet (and optionally
pivot/reshape the subsets), I can easily expand one spreadsheet into over 9000.
This is why the dataset count figure is near useless.

\subsection{Downloads of datasets}
What other measures can we use to asses how large a collection of datasets is?
Socrata Open Data Portal software provides a number for
how many times people have downloaded each dataset.
So how many have people downloaded these datasets?

![Plot of dataset counts and download counts by site](/dada/socrata-summary/figure/big_portals_density_text.png)

Here, the x axis is the same as the previous plot but the y axis is the total
downloads by site.

\subsection{Size of the datasets}
I can also look at how big they are.
It turns out that most of them are pretty small.

\begin{itemize}
\item Only 25% of datasets had more than 100 rows.
\item Only 12% of datasets had more than 1,000 rows.
\item Only 5% of datasets had more than 10,000 rows.
\end{itemize}

%   s <- read.csv('~/t/socrata-analysis/socrata-deduplicated.csv')
%   sum(is.na(s$nrow))                                                                                                  
% [1] 476
%   mean(s$nrow > 100, na.rm = T)                                                                                       
% [1] 0.2507369
%   mean(s$nrow > 1000, na.rm = T)                                                                                      
% [1] 0.1162797
%   mean(s$nrow > 10000, na.rm = T)                                                                                     
% [1] 0.05230904
%   mean(s$nrow > 100000, na.rm = T)                                                                                    
% [1] 0.01799659

Regardless of the format of these datasets, you can think of them as
spreadsheets without code, where columns are variables and rows are records.



\subsection{Derived datasets}
The Socrata product allows the public to write queries on datasets and
save them publically as new datasets. Aside from inflating the number
of datasets, making it hard to distinguish between official data and
unofficial queries on official data, and complicating the various
statistics about each dataset, this allows us to see how people are
querying datasets.

XXX image of families




\section{Measuring how well different spreadsheets follow data publishing guidelines}
Having gotten some feel for the contents of these various data catalogs,
I started running some less arbitrary statistics.
As discussed in section \ref{guidelines}, many groups have written guidelines
as to how data should be published.
\cite{open-data-census,fivestars,sunlight,sebastopol,odi}
I started coming up with measures of adherence to these guidelines and running
them across all of these datasets.










\section{Searching for spreadsheets}
I noticed that it's very hard to find spreadsheets that are relevant
to a particular analysis unless you already know that the spreadsheet exists.
Major search engines focus on HTML format web pages, and spreadsheet files
are often not indexed at all. The various data catalog software programs
discussed in section \ref{acquire} include a search feature, but this feature
only works within the particular website. For example, I have to go to the
Dutch government's data catalog website in order to search for Dutch data.

To summarize my thoughts about the common means of searching through
spreadsheets, I see two main issues.
The first issue is that the search is localized to datasets that are published
or otherwise managed by a particular entity; it's hard to search for
spreadsheets without first identifying a specific publisher or repository.
The second issue is that the search method is quite naive; these websites are
usually running crude keyword searches.

Having articulated these difficulties in searching for spreadsheets, I started
trying to address them.

\subsection{Searching across publishers}
When I'm looking for spreadsheets, the publishing organization is unlikely
to be my main concern. For example, if I'm interested in data about the
composition of different pesticides, but I don't really care whether the
data were collected by this city government or by that country government.


[![](/dada/openprism/taco.png)](http://openprism.thomaslevine.com)

And that's why I made OpenPrism. This is a disgustingly simple site that
forwards your search query to 100 other sites that house spreadsheets.

\subsection{Spreadsheets-specific search algorithms}
The other issue is that our search algorithms don't take advantage of all
of the structure that is encoded in a spreadsheet.

I started to address the first issue by pulling schema-related features out
of the spreadsheets.

XXX

Taking this further, I've been thinking
about what it would mean to have a search engine for spreadsheets.

![](wordsearch.png)

When we search for ordinary written documents, we send words into a search
engine and get pages of words back.

![](commasearch.png)

What if we could search for spreadsheets
by sending spreadsheets into a search engine and getting spreadsheets back?
The order of the results would be determined by various specialized statistics;
just as we use PageRank to find relevant hypertext documents, we can develop
other statistics that help us find relevant spreadsheets.

So now I'm looking for ways to do interesting searches on spreadsheets.

> Rows and columns

I think a lot about rows and columns. When we define tables in relational
databases, we can say reasonably well what each column means, based on
names and types, and what a row means, based on unique indices.
In spreadsheets, we still have column names, but we don't get everything
else.

The unique indices tell us quite a lot; they give us an idea about the
observational unit of the table and what other tables we can nicely
join or union with that table. So I made a package that finds unique
indices in ordinary CSV files.

    pip3 install special_snowflake

It's called "special snowflake", but it needs a better name.

If we pass the iris dataset to it, ::

    "Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    5.1,3.5,1.4,0.2,"setosa"
    4.9,3,1.4,0.2,"setosa"
    4.7,3.2,1.3,0.2,"setosa"
    4.6,3.1,1.5,0.2,"setosa"
    ...

we get no unique keys ::

    >>> special_snowflake.fromcsv(open('iris.csv'))                                                                  
    set()

because no combination of columns uniquely identifies the rows.
Of course, if we add an identifier column, ::

    "Id","Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    1,5.1,3.5,1.4,0.2,"setosa"
    2,4.9,3,1.4,0.2,"setosa"
    3,4.7,3.2,1.3,0.2,"setosa"
    4,4.6,3.1,1.5,0.2,"setosa"
    ...

that one gets returned. ::

    >>> special_snowflake.fromcsv(open('iris.csv'))                                                                  
    {('Id',)}

For a more interesting example, let's look at chickweight.

    "weight","Time","Chick","Diet"
    42,0,"1","1"
    51,2,"1","1"
    59,4,"1","1"
    64,6,"1","1"
    76,8,"1","1"
    ...

I could read the documentation on this dataset and tell you
what its statistical unit is (`?ChickWeight` in R), or I could
just let `special_snowflake` figure it out for me.

    >>> special_snowflake.fromcsv(open('chick.csv'))
    {('Time', 'Chick')}

The statistical unit is chicks in time. That is, something was
observed across multiple chick, and multiple observations were
taken from each (well, at least one) chick.

Some spreadsheets are have less obvious identifiers. In this
table of 1219 rows and 33 columns,

    >>> from urllib.request import urlopen
    >>> url = 'http://data.iledefrance.fr/explore/dataset/liste-des-points-de-contact-du-reseau-postal-dile-de-france/download/?format=csv'
    >>> fp = urlopen(url)
    >>> special_snowflake.fromcsv(fp, delimiter = ';')
    {('adresse', 'code_postal'),
     ('adresse', 'localite'),
     ('identifiant',),
     ('libelle_du_site',),
     ('wgs84',)}

we find five functional unique keys. Just by looking at the column names,
I'm gussing that the first two are combinations of parts of the postal address
and that the latter three look are formal identifiers.
And when I do things correctly and look at the
[data dictionary](http://data.iledefrance.fr/api/datasets/1.0/liste-des-points-de-contact-du-reseau-postal-dile-de-france/attachments/laposte_description_champs_pointdecontact_pdf/),
I come to the same interpretation.

This tells me that this dataset is about postal service locations,
with one location per row. It also gives me some ideas as to things that can
act as unique identifiers for postal service locations.

It's kind of cool to run this on individual spreadsheets, but it's even cooler
to run this on lots of spreadsheets.
In [blizzard](https://pypi.python.org/pypi/blizzard), I find spreadsheets with
the same unique indices, and then I look for overlap between those spreadsheets.

![](network.png)

Let's now trace what happens when we compare one spreadsheet to our database
of spreadsheets.

    # This is the one missing slide;
    # I'll fill it in once I make this thing easier to query.

Spreadsheets with high overlap might be good to join to each other, and
spreadsheets with low overlap might be good to union with each other.

All of this is quite crude at the moment, so I'm somewhat surprised that
anything interesting comes out.

## Review
I've been downloading lots of spreadsheets and doing crude, silly things
with them.

![](/dada/zombie-links/figure/p_prop_links.png)

I started out by looking at very simple things like how big they are.
I also tried to quantify other people's ideas of how good datasets are,
like whether they are up-to-date and whether they are freely licensed.

> ???
> ====

The main thing that I keep seeing is that nobody has any idea what's in all
of these spreadsheets; I notice this because people continue to be interested
in results that I find pretty boring.

> "Search"

Part of this is that it's pretty hard to search for spreadsheets.

The first issue is that you need to type your search into multiple search
bars. Dedicated search engines like DuckDuckGo don't index all of the
spreadsheets, so you're stuck using site-specific searches, and these only
search within their specific sites.

[![](/dada/openprism/taco.png)](http://openprism.thomaslevine.com)

I wrote OpenPrism to deal with this; you type your search into one search
bar, and it's as if you typed it in to all of the data catalogs I know about
at once.

The other issue is that our methods for searching spreadsheets are quite naive.
Most of these data catalogs look for exact string matches in the datasets or
for something similarly crude.

![](wordsearch.png)

The way we look up information today is to type some words into a search engine
and read the first few results. Why is that not the way we look up data?

We have designed search engines to look for things arranged in paragraphs
and sections and pages and hyperlinks; we haven't designed search engines for
things with tables and rows and columns and join keys.
PageRank is hailed as the magical invention that
fixed web search, but it depends on hyperlinks, so doesn't help much with spreadsheets.

![](commasearch.png)

The search engine for spreadsheets is going to look a bit different from
the search engine for words.















\section{Findings}
Here are some of the things I've found by looking at lots of spreadsheets at
once. I think of them as ways of automating some of the early steps in data
analysis, but I'm packaging them into the three categories I mentioned above
(looking for datasets, dealing with metadata problems, and quantifying quality).

\subsection{New ways of looking for datasets}
We search for prose by typing prose into a search bar; why don't
we search for spreadsheets by typing spreadsheets into a search bar?
Spreadsheets are much more structured than arbitrary prose, and we
can use this structure to enable new search paradigms. We could search
for things like the following.

\begin{itemize}
\item Spreadsheets that were produced by the same program as another spreadsheet
\item Spreadsheets that I can join to a particular spreadsheet
\item Spreadsheets with a particular statistical unit
\item Spreadsheets in long format (rather than wide format)
\end{itemize}

One example is the detection of spreadsheets that can be stacked on top of
each other (unioned).
My work on this started with AppGen,\cite{appgen} which was a system to
generate random apps based on randomly combined datasets. I combined spreadsheets
by matching spreadsheets with the same column headers.

Spreadsheets with the same column headers seemed to be semantically related to
each other. For example, there were 23 spreadsheets with these same columns.
\begin{itemize}
\item type\_of\_abuse\_of\_authority\_allegation
\item substantiated\_number
\item sunstantiated\_rate
\item exonerated\_number
\item exonerated\_rate
\item unsubstantiated\_number
\item unsubstantiated\_rate
\item unfounded\_number
\item unfounded\_rate
\item officer\_unidentified\_number
\item officer\_unidentified\_rate
\item miscellaneous
\item miscellaneous\_rate
\end{itemize}

All of these spreadsheets were uploaded by the same person, and they all had
titles of the form ``\$\{crime\} Allegations \$\{year\}'', such as
``Disposition Of Force Allegations 2006''.

When we organize spreadsheets by their column headers, groups of related
spreadsheets pop out at us.

\subsection{Dealing with incomplete metadata}
People complain about how data are bad and metadata are bad. Rather than
fixing it on a case-by-case basis, I think we should just come up with ways
of dealing with it. As an example, let's talk about unique identifiers.

I don't know of anyone who specifies within a spreadsheet file which columns
should be unique. There are methods with external metadata files, such as data
packages \cite{datapackages}, but hardly anyone uses those either. Rather than
trying to get people to write down which columns are unique, we can just figure
it out for ourselves.

special\_snowflake \cite{specialsnowflake} is a package that does just that.
It looks at all combinations of columns within a particular spreadsheet and
determines which combinations function as unique identifiers.

With tools like special\_snowflake, we don't have to rely as much on other
people for the creation of accurate metadata; we can lazily figure out the
metadata ourselves.

\subsection{Quantifying data quality}
A bunch of people \cite{open-data-census,fivestars,sunlight,sebastopol,odi}
have come up with relatively qualitative guidelines for publishing data.

I've been exploring ways of assigning numbers to represent data quality.
Using only the simple metadata files from open data catalogs, I've come up
with automated approaches for describing the updating, \cite{updatedness}
licensing, \cite{licensing} size \cite{summary}, file format \cite{file-formats}
and availability \cite{dead,zombie} of groups of datasets.

By quantifying these guidelines about data quality, we can more quickly and
precisely assess data quality.

\section{Applications}
A couple of people can share a few spreadsheets without any special means,
but it gets hard when there are more than a couple people sharing more than
a few spreadsheets. In my research, I'm coming up with approaches for assisting
this sharing.

Software for the publishing and analysis of data can integrate new search
paradigms to assist people in finding relevant datasets or enriching existing
datasets.

The ubiquitous and persistent complaint of bad metadata can be tempered through
the use of tools that infer metadata or provide alternative search strategies;
by becoming more robust to bad metadata, we make available a broader range of
datasets.

Quantification of the quality of data can be helpful to those who are tasked
with cataloging and maintaining a diverse array of datasets. Data quality
statistics provide a quick and timely summary of the issues with different
datasets and allow for a more targeted approach in the maintenance of a
data catalog.

\section{Future}
Many people say that releasing open data will create transparency in government,
engage citizens in their governments, and stimulate the economy. This all seems
reasonable, but I haven't seen much empirical research that suggests that the
sharing of data affects any of these outcomes.

As I come up with more ways of computationally describing datasets, I am becoming
more able to apply various quantitative analyses to the dataset of datasets;
this is starting to enable weaker versions of the aforementioned measure of
outcomes. It would be nice to know whether releasing your organization's data on
the internet will get people to use them.

\bibliographystyle{abbrv}
\bibliography{spreadsheets}
\balancecolumns
\end{document}
